# One Page Written Response 
In this model we used the Iris Dataset. This data is using the sepal and petal length to identify between three different species of the Iris flowers: Sentosa, Virginica, Versicolor. This dataset in particular has 120 flowers with sepal and petal measurements. 
To create the tf.dataset we used function tf.dataexperimental.make_csv_dataset to format the data. Then we used the function make_csv_dataset to receive a dataset with features as a dictionary that will help later on during modeling. We specified the model's input shape by using parameters that corresponded to the number of features from the dataset. The ideal number of layers and other functions such as neurons depends on the problem and dataset. 
It often has to played around with; the more layers available the more powerful the model which produces more effective results. For this dataset and problem, we used two layers with 10 nodes each. The training for this dataset is supervised machine learning model since our model contains species labels. 
Our model will calculate the loss using the tf.keras.losses.SparseCategoricalCrossentropy which takes the model's class probability predictions and desired labels to return the average loss. 
When we optimize, we attempt to minimize our model's loss function. We want to minimize our model's loss function because the lower the loss the better the model's prediction. 
In this model we used the tf.keras.optimizer.SGD. This is a hyperparameter that we can adjust to achieve within our model. We will be using num_epochs variable as the number of times to loop over a dataset. 
While running a longer model does not guarantee a better model, we can change this hyperparameter to choose the best number. We will use matplotlib to visualize the loss as well as the Epoch and the accuracy, so we have a better idea about what number in our hyperparameter works best. 
This helps us to estimate loss as well since we want to see the loss go down as the accuracy goes up. To evaluate our model, we provided the model with some testing data and asked it to predict the species of Iris using the model we trained. 
We compared the model predictions to the actual label of the flowers (we could do this since this was a supervised machine learning problem). We can divide number of correct predictions divided by number of predictions total and get an accuracy percentage. We made new predictions on our good model by providing it with three unlabeled examples to see if it could accurately predict the new data. 
We see that for two of the three test data results the model was very confident in its predictive power. While for the last example the model was not as confident as the first two examples it still found a very strong correlation between the unknown data and what it saw in the training dataset.
The results for each example were: 1= 98.1%, 2= 93%, 3= 81.3%. 